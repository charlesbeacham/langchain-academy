{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0ebaf1",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-2/trim-filter-messages.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239435-lesson-4-trim-and-filter-messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ea2f9-03ff-4647-b782-46867ebed04e",
   "metadata": {},
   "source": [
    "# Filtering and trimming messages\n",
    "\n",
    "## Review\n",
    "\n",
    "Now, we have a deeper understanding of a few things: \n",
    "\n",
    "* How to customize the graph state schema\n",
    "* How to define custom state reducers\n",
    "* How to use multiple graph state schemas\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we can start using these concepts with models in LangGraph!\n",
    " \n",
    "In the next few sessions, we'll build towards a chatbot that has long-term memory.\n",
    "\n",
    "Because our chatbot will use messages, let's first talk a bit more about advanced ways to work with messages in graph state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768dc606-d5f2-468d-96ea-910b264e0f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b64d8d3-e4ac-4961-bdc0-688825eb5864",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing).\n",
    "\n",
    "We'll log to a project, `langchain-academy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd020c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72f3fc90-58b6-4f7f-897e-dddf6ae532c7",
   "metadata": {},
   "source": [
    "## Messages as state\n",
    "\n",
    "First, let's define some messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11a463-e27a-4a05-b41d-64882e38edca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b814adcb-6bf9-4b75-be11-e59f933fbd0c",
   "metadata": {},
   "source": [
    "Recall we can pass them to a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712e288-e622-48a2-ad3f-a52f65f3ab08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbd1dab8-0af8-4621-8264-ce65065f76ec",
   "metadata": {},
   "source": [
    "We can run our chat model in a simple graph with `MessagesState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8c39c-633b-4176-9cc6-8318e42bb5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a3e4a-ccfd-4d14-81f1-f0de6e11a1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34c33e63-1ef4-412d-bb10-6a1b9e5b35a7",
   "metadata": {},
   "source": [
    "## Reducer\n",
    "\n",
    "A practical challenge when working with messages is managing long-running conversations. \n",
    "\n",
    "Long-running conversations result in high token usage and latency if we are not careful, because we pass a growing list of messages to the model.\n",
    "\n",
    "We have a few ways to address this.\n",
    "\n",
    "First, recall the trick we saw using `RemoveMessage` and the `add_messages` reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c6bc5-bb0e-4a43-80f5-c8ec38d99f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7c2cc-54ce-43e7-9a90-abf37827d709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f506457d-014b-4fee-a684-e5edfb4b8f0d",
   "metadata": {},
   "source": [
    "## Filtering messages\n",
    "\n",
    "If you don't need or want to modify the graph state, you can just filter the messages you pass to the chat model.\n",
    "\n",
    "For example, just pass in a filtered list: `llm.invoke(messages[-1:])` to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0b904-7cd6-486b-8948-105bee3d4683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f58c6fc-532f-418d-b70a-cfcb3307daf5",
   "metadata": {},
   "source": [
    "Let's take our existing list of messages, append the above LLM response, and append a follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16956015-1dbe-4108-89b5-4209b68b51ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85563415-c085-46a8-a4ac-155df798c54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23349705-a059-47b5-9760-d8f64e687393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42e1d8d2-e297-4d78-b54c-d12b3c866745",
   "metadata": {},
   "source": [
    "The state has all of the mesages.\n",
    "\n",
    "But, let's look at the LangSmith trace to see that the model invocation only uses the last message:\n",
    "\n",
    "https://smith.langchain.com/public/75aca3ce-ef19-4b92-94be-0178c7a660d9/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40d930-3c1f-47fe-8d2a-ce174873353c",
   "metadata": {},
   "source": [
    "## Trim messages\n",
    "\n",
    "Another approach is to [trim messages](https://python.langchain.com/v0.2/docs/how_to/trim_messages/#getting-the-last-max_tokens-tokens), based upon a set number of tokens. \n",
    "\n",
    "This restricts the message history to a specified number of tokens.\n",
    "\n",
    "While filtering only returns a post-hoc subset of the messages between agents, trimming restricts the number of tokens that a chat model can use to respond.\n",
    "\n",
    "See the `trim_messages` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff99b81-cf03-4cc2-b44f-44829a73e1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df63ac-da29-4874-b3df-7e390e97cc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d8971-c75c-43ca-a209-eb1d07b2ead0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70a269-a869-4fa0-a1df-29736a432c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38b3db67-380e-46b5-9a6a-20100ba52008",
   "metadata": {},
   "source": [
    "Let's look at the LangSmith trace to see the model invocation:\n",
    "\n",
    "https://smith.langchain.com/public/b153f7e9-f1a5-4d60-8074-f0d7ab5b42ef/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}