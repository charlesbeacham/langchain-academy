{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0e279f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/parallelization.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239934-lesson-1-parallelization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4169bfb-769a-4db3-833e-c827f19024b2",
   "metadata": {},
   "source": [
    "# Parallel node execution\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 3, we went in-depth on `human-in-the loop`, showing 3 common use-cases:\n",
    "\n",
    "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
    "\n",
    "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
    "\n",
    "(3) `Editing` - You can modify the state \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will build on `human-in-the-loop` as well as the `memory` concepts discussed in module 2.\n",
    "\n",
    "We will dive into `multi-agent` workflows, and build up to a multi-agent research assistant that ties together all of the modules from this course.\n",
    "\n",
    "To build this multi-agent research assistant, we'll first discuss a few LangGraph controllability topics.\n",
    "\n",
    "We'll start with [parallelization](https://langchain-ai.github.io/langgraph/how-tos/branching/#how-to-create-branches-for-parallel-node-execution).\n",
    "\n",
    "## Fan out and fan in\n",
    "\n",
    "Let's build a simple linear graph that over-writes the state at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618eab5c-4ef7-4273-8e0b-a9c847897ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbec0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd77093-1794-4bd7-8c57-58f59a74c20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdd027d3-ef9f-4d43-b190-e9f07d521e18",
   "metadata": {},
   "source": [
    "We over-write state, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf260088-90d5-45b2-93ab-42f241560840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9dec27d-dc43-4088-beb2-53ad090d2971",
   "metadata": {},
   "source": [
    "Now, let's run `b` and `c` in parallel. \n",
    "\n",
    "And then run `d`.\n",
    "\n",
    "We can do this easily with fan-out from `a` to `b` and `c`, and then fan-in to `d`.\n",
    "\n",
    "The the state updates are applied at the end of each step.\n",
    "\n",
    "Let's run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeaaab-a8c3-470f-89ef-9cf0a2760667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35238fde-0230-4ae8-9200-158a8835c4f1",
   "metadata": {},
   "source": [
    "**We see an error**! \n",
    "\n",
    "This is because both `b` and `c` are writing to the same state key / channel in the same step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048b041-6849-4f09-9811-6b7a80f67859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64cc329d-59fa-4c26-adcf-9122a824955d",
   "metadata": {},
   "source": [
    "When using fan out, we need to be sure that we are using a reducer if steps are writing to the same the channel / key. \n",
    "\n",
    "As we touched on in Module 2, `operator.add` is a function from Python's built-in operator module.\n",
    "\n",
    "When `operator.add` is applied to lists, it performs list concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1292ac-510a-4801-b2a3-e2c6d2d9582a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbad231-fc1d-49b1-a9fc-ed9153fa3977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf5baa2-cecd-44b6-b0c4-d258340783f8",
   "metadata": {},
   "source": [
    "Now we see that we append to state for the updates made in parallel by `b` and `c`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fc7c7-198d-41be-867f-e77c93ba3217",
   "metadata": {},
   "source": [
    "## Waiting for nodes to finish\n",
    "\n",
    "Now, lets consider a case where one parallel path has more steps than the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b5d4f-dd39-4c22-b623-e0abc23f9144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11640e6f-ac62-4ad4-89d9-7f6f9b56bf7a",
   "metadata": {},
   "source": [
    "In this case, `b`, `b2`, and `c` are all part of the same step.\n",
    "\n",
    "The graph will wait for all of these to be completed before proceeding to step `d`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafda930-e75b-410f-ba93-eb5fc0219303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6610a2e3-b053-47e8-bf4e-0968dfaa0a5d",
   "metadata": {},
   "source": [
    "## Setting the order of state updates\n",
    "\n",
    "However, within each step we don't have specific control over the order of the state updates!\n",
    "\n",
    "In simple terms, it is a deterministic order determined by LangGraph based upon graph topology that **we do not control**. \n",
    "\n",
    "Above, we see that `c` is added before `b2`.\n",
    "\n",
    "However, we can use a custom reducer to customize this e.g., sort state updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24788e73-0950-432e-ad32-7987ea076529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dba2e-f9f0-4bc7-8ba6-684521a49bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb1714c0-e881-48e7-bcb8-a60016f0485e",
   "metadata": {},
   "source": [
    "Now, the reducer sorts the updated state values!\n",
    "\n",
    "The `sorting_reducer` example sorts all values globally. We can also: \n",
    "\n",
    "1. Write outputs to a separate field in the state during the parallel step\n",
    "2. Use a \"sink\" node after the parallel step to combine and order those outputs\n",
    "3. Clear the temporary field after combining\n",
    "\n",
    "See the [docs](https://langchain-ai.github.io/langgraph/how-tos/branching/#stable-sorting) for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0750b-e6af-40d9-835c-c664da5a2d3b",
   "metadata": {},
   "source": [
    "## Working with LLMs\n",
    "\n",
    "Now, lets add a realistic example! \n",
    "\n",
    "We want to gather context from two external sources (Wikipedia and Web-Search) and have an LLM answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9d03c-cb41-415c-862d-c9616d5a2d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75cc78-d1a1-47a5-8648-bf5a79c883de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e714ea8-095c-461a-98bc-ee782a84ef5c",
   "metadata": {},
   "source": [
    "You can try different web search tools. [Tavily](https://tavily.com/) is one nice option to consider, but ensure your `TAVILY_API_KEY` is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb519a-d08a-4ec7-8f0b-2ce6a9bf7342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4f56c-3334-4927-8ed8-62fd384ee43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa544ca0-10af-491e-ad7a-477d004413eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dbbecab-80eb-4f0c-b43a-45542fc0ae9c",
   "metadata": {},
   "source": [
    "## Using with LangGraph API\n",
    "\n",
    "**‚ö†Ô∏è DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- üöÄ API: http://127.0.0.1:2024\n",
    "- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- üìö API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc8ad8d-1365-4801-a8a5-b85cd4965119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23919dc9-27d8-4d10-b91d-24acdf8c0fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35e68f-4017-4f45-93cf-ddbb355a0bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}